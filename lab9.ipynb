{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR with logistic regression\n",
    "logistic regression으로 xor문제를 풀면 값들을 정확히 분류하는 linear한 선을 못찾아서 제대로 학습이 안된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.5542297 [[ 0.0474427]\n",
      " [-1.5620832]]\n",
      "100 0.8165529 [[-0.73732084]\n",
      " [-1.7628918 ]]\n",
      "200 0.7520941 [[-0.63023543]\n",
      " [-1.1878258 ]]\n",
      "300 0.7207432 [[-0.4785249]\n",
      " [-0.7781207]]\n",
      "400 0.7058437 [[-0.3485411]\n",
      " [-0.5089461]]\n",
      "500 0.6989423 [[-0.24797751]\n",
      " [-0.33374527]]\n",
      "600 0.69578457 [[-0.17386368]\n",
      " [-0.21969923]]\n",
      "700 0.69434667 [[-0.12071796]\n",
      " [-0.14520784]]\n",
      "800 0.6936928 [[-0.08324724]\n",
      " [-0.09633098]]\n",
      "900 0.69339544 [[-0.05712307]\n",
      " [-0.06411282]]\n",
      "1000 0.6932602 [[-0.03905205]\n",
      " [-0.04278613]]\n",
      "1100 0.6931987 [[-0.02662279]\n",
      " [-0.02861761]]\n",
      "1200 0.6931706 [[-0.0181102 ]\n",
      " [-0.01917586]]\n",
      "1300 0.69315785 [[-0.01229886]\n",
      " [-0.01286815]]\n",
      "1400 0.69315207 [[-0.0083414 ]\n",
      " [-0.00864551]]\n",
      "1500 0.69314945 [[-0.00565155]\n",
      " [-0.00581402]]\n",
      "1600 0.6931482 [[-0.00382603]\n",
      " [-0.00391282]]\n",
      "1700 0.69314766 [[-0.00258855]\n",
      " [-0.00263491]]\n",
      "1800 0.69314736 [[-0.00175043]\n",
      " [-0.0017752 ]]\n",
      "1900 0.6931473 [[-0.00118322]\n",
      " [-0.00119646]]\n",
      "2000 0.6931472 [[-0.00079957]\n",
      " [-0.00080663]]\n",
      "2100 0.6931471 [[-0.00054019]\n",
      " [-0.00054395]]\n",
      "2200 0.6931471 [[-0.00036485]\n",
      " [-0.00036686]]\n",
      "2300 0.6931472 [[-0.00024643]\n",
      " [-0.0002475 ]]\n",
      "2400 0.6931472 [[-0.00016641]\n",
      " [-0.00016698]]\n",
      "2500 0.6931471 [[-0.00011235]\n",
      " [-0.00011266]]\n",
      "2600 0.6931472 [[-7.5854565e-05]\n",
      " [-7.6021373e-05]]\n",
      "2700 0.6931472 [[-5.1206556e-05]\n",
      " [-5.1294395e-05]]\n",
      "2800 0.69314724 [[-3.4560482e-05]\n",
      " [-3.4609569e-05]]\n",
      "2900 0.6931472 [[-2.3327981e-05]\n",
      " [-2.3354716e-05]]\n",
      "3000 0.69314724 [[-1.5743290e-05]\n",
      " [-1.5758103e-05]]\n",
      "3100 0.6931472 [[-1.06232483e-05]\n",
      " [-1.06306115e-05]]\n",
      "3200 0.6931472 [[-7.1676764e-06]\n",
      " [-7.1720592e-06]]\n",
      "3300 0.6931472 [[-4.8326651e-06]\n",
      " [-4.8340676e-06]]\n",
      "3400 0.6931471 [[-3.2635699e-06]\n",
      " [-3.2649723e-06]]\n",
      "3500 0.6931472 [[-2.2040933e-06]\n",
      " [-2.2054958e-06]]\n",
      "3600 0.6931472 [[-1.4649936e-06]\n",
      " [-1.4663960e-06]]\n",
      "3700 0.6931472 [[-1.0075236e-06]\n",
      " [-1.0089260e-06]]\n",
      "3800 0.6931471 [[-6.7969700e-07]\n",
      " [-6.8109944e-07]]\n",
      "3900 0.6931471 [[-4.3978815e-07]\n",
      " [-4.4119059e-07]]\n",
      "4000 0.6931472 [[-2.7736488e-07]\n",
      " [-2.7876732e-07]]\n",
      "4100 0.6931472 [[-2.1179952e-07]\n",
      " [-2.1320196e-07]]\n",
      "4200 0.6931472 [[-1.6262550e-07]\n",
      " [-1.6402794e-07]]\n",
      "4300 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "4400 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "4500 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "4600 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "4700 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "4800 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "4900 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "5000 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "5100 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "5200 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "5300 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "5400 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "5500 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "5600 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "5700 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "5800 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "5900 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "6000 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "6100 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "6200 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "6300 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "6400 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "6500 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "6600 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "6700 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "6800 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "6900 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "7000 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "7100 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "7200 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "7300 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "7400 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "7500 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "7600 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "7700 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "7800 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "7900 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "8000 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "8100 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "8200 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "8300 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "8400 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "8500 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "8600 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "8700 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "8800 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "8900 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "9000 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "9100 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "9200 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "9300 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "9400 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "9500 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "9600 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "9700 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "9800 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "9900 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "10000 0.6931472 [[-1.3133294e-07]\n",
      " [-1.3273538e-07]]\n",
      "\n",
      "Hypothesis:  [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]] \n",
      "Correct:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]] \n",
      "Accuracy:  0.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 2])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([2, 1]), name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "\n",
    "# Hypothesis using sigmoid\n",
    "# 0 ~ 1사이 값으로 만든다\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "\n",
    "# minimize cost\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# True if hypothesis > 0.5 else False\n",
    "# cast([True, False], tf.int32) => [1,0]\n",
    "# 예측 값들을 모두 1 또는 0 값으로 바꿨다\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "\n",
    "# Accuracy computation\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        _, cost_val, w_val = sess.run(\n",
    "                  [train, cost, W], feed_dict={X: x_data, Y: y_data}\n",
    "        )\n",
    "        if step % 100 == 0:\n",
    "            print(step, cost_val, w_val)\n",
    "\n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run(\n",
    "              [hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data}\n",
    "    )\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR with Neural Net\n",
    "logistic regression으로 수행했을 때 잘 나오지 않았던 정확도가 두 층으로 나눠서 실행하니 올라 갔다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.71640736\n",
      "100 0.69328487\n",
      "200 0.69280076\n",
      "300 0.6923225\n",
      "400 0.6917488\n",
      "500 0.6910291\n",
      "600 0.6900968\n",
      "700 0.68886656\n",
      "800 0.6872314\n",
      "900 0.68506557\n",
      "1000 0.68223065\n",
      "1100 0.678587\n",
      "1200 0.6740049\n",
      "1300 0.6683727\n",
      "1400 0.66160417\n",
      "1500 0.6536528\n",
      "1600 0.6445395\n",
      "1700 0.63438815\n",
      "1800 0.62344694\n",
      "1900 0.6120729\n",
      "2000 0.60067356\n",
      "2100 0.5896304\n",
      "2200 0.5792372\n",
      "2300 0.5696732\n",
      "2400 0.5610087\n",
      "2500 0.5532261\n",
      "2600 0.546244\n",
      "2700 0.53993154\n",
      "2800 0.53410697\n",
      "2900 0.5285151\n",
      "3000 0.52277064\n",
      "3100 0.51626956\n",
      "3200 0.50814146\n",
      "3300 0.4975105\n",
      "3400 0.48415878\n",
      "3500 0.4685408\n",
      "3600 0.45077986\n",
      "3700 0.4303946\n",
      "3800 0.40688732\n",
      "3900 0.38023797\n",
      "4000 0.3510564\n",
      "4100 0.32046282\n",
      "4200 0.28981102\n",
      "4300 0.26037365\n",
      "4400 0.23310408\n",
      "4500 0.20854503\n",
      "4600 0.18686634\n",
      "4700 0.16797444\n",
      "4800 0.1516259\n",
      "4900 0.13751534\n",
      "5000 0.12533091\n",
      "5100 0.11478351\n",
      "5200 0.10561861\n",
      "5300 0.09761882\n",
      "5400 0.09060173\n",
      "5500 0.08441562\n",
      "5600 0.07893463\n",
      "5700 0.07405464\n",
      "5800 0.069689505\n",
      "5900 0.06576718\n",
      "6000 0.062227912\n",
      "6100 0.059021406\n",
      "6200 0.056105323\n",
      "6300 0.05344408\n",
      "6400 0.051007196\n",
      "6500 0.048768714\n",
      "6600 0.046706352\n",
      "6700 0.044801064\n",
      "6800 0.04303615\n",
      "6900 0.041397348\n",
      "7000 0.039872043\n",
      "7100 0.038449258\n",
      "7200 0.03711933\n",
      "7300 0.03587377\n",
      "7400 0.034705028\n",
      "7500 0.03360637\n",
      "7600 0.03257195\n",
      "7700 0.0315964\n",
      "7800 0.030675054\n",
      "7900 0.029803548\n",
      "8000 0.028978117\n",
      "8100 0.02819525\n",
      "8200 0.027451808\n",
      "8300 0.026745018\n",
      "8400 0.026072223\n",
      "8500 0.025431165\n",
      "8600 0.024819672\n",
      "8700 0.024235807\n",
      "8800 0.023677744\n",
      "8900 0.023143845\n",
      "9000 0.022632629\n",
      "9100 0.022142718\n",
      "9200 0.021672845\n",
      "9300 0.021221783\n",
      "9400 0.020788513\n",
      "9500 0.020371944\n",
      "9600 0.019971235\n",
      "9700 0.019585438\n",
      "9800 0.01921379\n",
      "9900 0.018855546\n",
      "10000 0.018509964\n",
      "\n",
      "Hypothesis:\n",
      "[[0.01447789]\n",
      " [0.9800071 ]\n",
      " [0.9800355 ]\n",
      " [0.0188998 ]] \n",
      "Predicted:\n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 2])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# shape \n",
    "# [입력,출력] : x 2개가 합쳐져서 x 2개가 입력으로 들어 가고 출력도 2개(다음거에 입력으로 2개가 들어 가야 하니까)로 정한다\n",
    "W1 = tf.Variable(tf.random_normal([2, 2]), name='weight1')\n",
    "b1 = tf.Variable(tf.random_normal([2]), name='bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "# shape\n",
    "# layer1의 출력 2개가 입력으로 들어 가고 y데이터처럼 출력을 1로 정한다\n",
    "W2 = tf.Variable(tf.random_normal([2, 1]), name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([1]), name='bias2')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        _, cost_val = sess.run([train, cost], feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 100 == 0:\n",
    "            print(step, cost_val)\n",
    "\n",
    "    # Accuracy report\n",
    "    h, p, a = sess.run(\n",
    "        [hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data}\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nHypothesis:\\n{h} \\nPredicted:\\n{p} \\nAccuracy:\\n{a}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wide&deep NN for XOR\n",
    "모델이 더 정확하게 학습된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.2353868\n",
      "100 0.69231355\n",
      "200 0.6878969\n",
      "300 0.68259275\n",
      "400 0.6755543\n",
      "500 0.6656918\n",
      "600 0.6518531\n",
      "700 0.63300496\n",
      "800 0.6079143\n",
      "900 0.5746501\n",
      "1000 0.5303893\n",
      "1100 0.47042334\n",
      "1200 0.3871851\n",
      "1300 0.28343183\n",
      "1400 0.19569892\n",
      "1500 0.13690907\n",
      "1600 0.09904842\n",
      "1700 0.07446556\n",
      "1800 0.058010787\n",
      "1900 0.046583585\n",
      "2000 0.03836023\n",
      "2100 0.03225133\n",
      "2200 0.027586706\n",
      "2300 0.023939695\n",
      "2400 0.021030053\n",
      "2500 0.018667765\n",
      "2600 0.016720738\n",
      "2700 0.0150946155\n",
      "2800 0.0137207545\n",
      "2900 0.012548004\n",
      "3000 0.011537703\n",
      "3100 0.01066019\n",
      "3200 0.009892391\n",
      "3300 0.009216054\n",
      "3400 0.008616613\n",
      "3500 0.008082406\n",
      "3600 0.007603811\n",
      "3700 0.0071731256\n",
      "3800 0.0067837914\n",
      "3900 0.0064304657\n",
      "4000 0.0061085243\n",
      "4100 0.005814295\n",
      "4200 0.0055444404\n",
      "4300 0.0052962457\n",
      "4400 0.005067254\n",
      "4500 0.0048554447\n",
      "4600 0.0046590855\n",
      "4700 0.004476566\n",
      "4800 0.004306562\n",
      "4900 0.004147869\n",
      "5000 0.0039994502\n",
      "5100 0.003860373\n",
      "5200 0.0037297967\n",
      "5300 0.0036070743\n",
      "5400 0.0034914848\n",
      "5500 0.0033824432\n",
      "5600 0.0032794687\n",
      "5700 0.003182066\n",
      "5800 0.0030898\n",
      "5900 0.003002311\n",
      "6000 0.0029192385\n",
      "6100 0.0028402824\n",
      "6200 0.0027651736\n",
      "6300 0.0026935814\n",
      "6400 0.0026253415\n",
      "6500 0.002560184\n",
      "6600 0.002497944\n",
      "6700 0.0024384116\n",
      "6800 0.0023814524\n",
      "6900 0.0023268862\n",
      "7000 0.0022745633\n",
      "7100 0.002224379\n",
      "7200 0.0021761986\n",
      "7300 0.0021298872\n",
      "7400 0.002085385\n",
      "7500 0.0020425722\n",
      "7600 0.002001299\n",
      "7700 0.0019615658\n",
      "7800 0.0019232823\n",
      "7900 0.001886359\n",
      "8000 0.0018507061\n",
      "8100 0.0018163084\n",
      "8200 0.001783002\n",
      "8300 0.001750861\n",
      "8400 0.0017197808\n",
      "8500 0.0016897168\n",
      "8600 0.0016605791\n",
      "8700 0.0016323676\n",
      "8800 0.0016050228\n",
      "8900 0.0015785296\n",
      "9000 0.0015528578\n",
      "9100 0.0015279031\n",
      "9200 0.0015037252\n",
      "9300 0.0014802045\n",
      "9400 0.0014574007\n",
      "9500 0.0014351944\n",
      "9600 0.0014136602\n",
      "9700 0.0013926785\n",
      "9800 0.001372309\n",
      "9900 0.0013524325\n",
      "10000 0.0013330936\n",
      "\n",
      "Hypothesis:  [[1.6921833e-03]\n",
      " [9.9864727e-01]\n",
      " [9.9860352e-01]\n",
      " [8.8654709e-04]] \n",
      "Correct:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 2])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([2, 10]), name='weight1')\n",
    "b1 = tf.Variable(tf.random_normal([10]), name='bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([10, 10]), name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([10]), name='bias2')\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([10, 10]), name='weight3')\n",
    "b3 = tf.Variable(tf.random_normal([10]), name='bias3')\n",
    "layer3 = tf.sigmoid(tf.matmul(layer2, W3) + b3)\n",
    "\n",
    "W4 = tf.Variable(tf.random_normal([10, 1]), name='weight4')\n",
    "b4 = tf.Variable(tf.random_normal([1]), name='bias4')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer3, W4) + b4)\n",
    "\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        _, cost_val = sess.run([train, cost], feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 100 == 0:\n",
    "            print(step, cost_val)\n",
    "\n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run(\n",
    "        [hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data}\n",
    "    )\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
